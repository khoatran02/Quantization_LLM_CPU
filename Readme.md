# Quantization LLM for CPU
## Overview

This project offers a complete framework for quantizing Large Language Models (LLMs) to enhance CPU inference efficiency. It features an API for serving quantized models, a Jupyter notebook for experimentation, and pre-quantized model examples.

### âœ¨ Features

- **Model Quantization:** Supports 4-bit and 8-bit quantization to reduce model size and speed up inference.
- **API Services:** Provides a robust API for serving and interacting with quantized models.
- **Dockerized Deployment:** Includes a Dockerfile for streamlined containerized deployment.
- **Modular Architecture:** Organized codebase with clear separation of controllers, services, and utilities.
- **Testing Included:** Scripts for validating API functionality.

### ğŸ“‚ Project Structure

```
Quantization_LLM_CPU/
â”œâ”€â”€ Quantizing_LLM.ipynb
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ request.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ constants.py
â”‚   â”‚   â”œâ”€â”€ helpers.py
â”‚   â”‚   â”œâ”€â”€ model_utils.py
â”‚   â”‚   â””â”€â”€ response_common.py
â”‚   â”œâ”€â”€ controller/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ model_controller.py
â”‚   â”œâ”€â”€ llm_models/
â”‚   â”‚   â”œâ”€â”€ 4bit/
â”‚   â”‚   â”‚   â””â”€â”€ Qwen3-8B_FP16_Q4_K_M.gguf
â”‚   â”‚   â””â”€â”€ 8bit/
â”‚   â”‚       â””â”€â”€ Qwen3-8B_FP16_Q8_0.gguf
â”‚   â””â”€â”€ service/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ model_service.py
â”‚       â””â”€â”€ quantization_service.py
â””â”€â”€ README.md
```

### ğŸ› ï¸ Component Breakdown

- **Quantizing_LLM.ipynb:** Jupyter notebook for experimenting with quantization techniques and evaluating performance.
- **app/**: Main application directory.
    - **main.py:** API server entry point.
    - **request.py:** API request structures.
    - **requirements.txt:** Python dependencies.
    - **test_api.py:** API endpoint tests.
    - **Dockerfile:** Docker image build instructions.
- **common/**: Shared utilities and constants.
    - **constants.py:** Configuration values.
    - **helpers.py:** Helper functions.
    - **model_utils.py:** LLM interaction utilities.
    - **response_common.py:** Common API responses.
- **controller/**: Business logic.
    - **model_controller.py:** LLM operations management.
- **llm_models/**: Pre-quantized models.
    - **4bit/**: 4-bit models.
    - **8bit/**: 8-bit models.
- **service/**: Model management and quantization logic.
    - **model_service.py:** LLM loading and serving.
    - **quantization_service.py:** Model quantization logic.

### ğŸš€ Getting Started

**Prerequisites:**  
- Python 3.10+  
- pip

**Install dependencies:**
```bash
pip install -r app/requirements.txt
```

**Run the application:**
```bash
python app/main.py
```

**Test the API:**
```bash
python app/test_api.py
```

**Experiment with quantization:**
```bash
jupyter notebook Quantizing_LLM.ipynb
```

### ğŸ“œ License

Licensed under the MIT License.
